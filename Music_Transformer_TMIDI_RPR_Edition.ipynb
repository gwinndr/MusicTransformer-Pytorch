{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music_Transformer_TMIDI_RPR_Edition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9opKSK2RSDRg"
      },
      "source": [
        "# Super Piano 3: Google Music Transformer Reproduction\n",
        "\n",
        "## Generating Music with Long-Term structure\n",
        "\n",
        "## Powered by TMIDI 2.0. RPR Processors\n",
        "## https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "### Based on 2019 ICLR paper by Cheng-Zhi Anna Huang, Google Brain and Damon Gwinn's code/repo https://github.com/gwinndr/MusicTransformer-Pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05hD19W0hSCP"
      },
      "source": [
        "###Setup Environment and Dependencies. Check GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ror_UJUp7wlO",
        "cellView": "form"
      },
      "source": [
        "#@title Check if GPU (driver) is avaiiable (you do not want to run this on CPU, trust me)\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paYvoZHihtux",
        "cellView": "form"
      },
      "source": [
        "#@title Clone/Install all dependencies\n",
        "!git clone https://github.com/Tegridy-Code/MusicTransformer-Pytorch\n",
        "!git clone https://github.com/asigalov61/tegridy-tools\n",
        "!cp /content/tegridy-tools/tegridy-tools/TMIDI.py /content/MusicTransformer-Pytorch\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install progress\n",
        "!pip install pretty-midi\n",
        "!pip install pypianoroll\n",
        "!pip install matplotlib\n",
        "!pip install librosa\n",
        "!pip install scipy\n",
        "!pip install pillow\n",
        "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
        "!pip install midi2audio\n",
        "!pip install mir_eval\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 /content/font.sf2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM71tUPVfffi",
        "cellView": "form"
      },
      "source": [
        "#@title Import all needed modules\n",
        "%cd /content/tegridy-tools/tegridy-tools/\n",
        "import TMIDI\n",
        "%cd /content/\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import pretty_midi as pyd\n",
        "from pretty_midi import Note\n",
        "from pprint import pprint\n",
        "\n",
        "# For plotting\n",
        "import pypianoroll\n",
        "from pypianoroll import Multitrack, Track\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import mir_eval.display\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "# For rendering output audio\n",
        "import pretty_midi\n",
        "from midi2audio import FluidSynth\n",
        "from google.colab import output\n",
        "from IPython.display import display, Javascript, HTML, Audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKz4SKoeXYWc"
      },
      "source": [
        "# Process your own custom MIDI DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XlPWN0aQhB42"
      },
      "source": [
        "#@title Create IO dirs\n",
        "\n",
        "!mkdir /content/MusicTransformer-Pytorch/dataset/e_piano\n",
        "!mkdir /content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis\n",
        "!mkdir /content/MusicTransformer-Pytorch/dataset/e_piano/test\n",
        "!mkdir /content/MusicTransformer-Pytorch/dataset/e_piano/train\n",
        "!mkdir /content/MusicTransformer-Pytorch/dataset/e_piano/val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUXaozztvGU2",
        "cellView": "form"
      },
      "source": [
        "#@title Upload your custom MIDI DataSet to created \"dataset/e_piano/custom_midis\" folder through this cell or manually through any other means. You can also use the dataset below.\n",
        "from google.colab import files\n",
        "%cd '/content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDsmK-vkwOgl",
        "cellView": "form"
      },
      "source": [
        "#@title Tegridy Piano Dataset (~230 Modern Piano MIDIs)\n",
        "%cd /content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis\n",
        "!wget 'https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Tegridy-Piano-CC-BY-NC-SA.zip'\n",
        "!unzip -j 'Tegridy-Piano-CC-BY-NC-SA.zip'\n",
        "!rm Tegridy-Piano-CC-BY-NC-SA.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5L1y614whAo",
        "cellView": "form"
      },
      "source": [
        "#@title MAESTRO 3.0 MIDI dataset\n",
        "%cd /content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis\n",
        "!wget 'https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip'\n",
        "!unzip -j 'maestro-v3.0.0-midi.zip'\n",
        "!rm maestro-v3.0.0-midi.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Yigmx40Uh3",
        "cellView": "form"
      },
      "source": [
        "#@title Tegridy Piano Transformer Dataset\n",
        "%cd /content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis\n",
        "!wget 'https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Tegridy-Piano-Transformer-Dataset-CC-BY-NC-SA.zip'\n",
        "!unzip -j 'Tegridy-Piano-Transformer-Dataset-CC-BY-NC-SA.zip'\n",
        "!rm Tegridy-Piano-Transformer-Dataset-CC-BY-NC-SA.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mKBs9XgnPQs7"
      },
      "source": [
        "#@title Syntethic Tegridy Piano Dataset dataset\n",
        "%cd /content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis\n",
        "!wget 'https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Synthetic-Tegridy-Piano-MIDI-Dataset-CC-BY-NC-SA.zip'\n",
        "!unzip 'Synthetic-Tegridy-Piano-MIDI-Dataset-CC-BY-NC-SA.zip'\n",
        "!rm Synthetic-Tegridy-Piano-MIDI-Dataset-CC-BY-NC-SA.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-jAV_Qv5Fn_"
      },
      "source": [
        "For now, we are going to split the dataset by random into \"test\"/\"val\" dirs which is not ideal. So feel free to modify the code to your liking to achieve better training results with this implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "m0jpYxcRRr2B"
      },
      "source": [
        "#@title Declare Notes Representation (RPR) processor and helper functions\n",
        "\n",
        "'''\n",
        "This is the data processing script\n",
        "============\n",
        "It will allow you to quickly process the MIDI Files into the Google Magenta's music representation \n",
        "    as like [Music Transformer](https://magenta.tensorflow.org/music-transformer) \n",
        "            [Performance RNN](https://magenta.tensorflow.org/performance-rnn).\n",
        "\n",
        "'''\n",
        "\n",
        "total = 0\n",
        "def process_midi(path):\n",
        "    global total\n",
        "    data = pyd.PrettyMIDI(path)\n",
        "    main_notes = []\n",
        "    acc_notes = []\n",
        "    for ins in data.instruments:\n",
        "        acc_notes.extend(ins.notes)\n",
        "    for i in range(len(main_notes)):\n",
        "        main_notes[i].start = round(main_notes[i].start,2)\n",
        "        main_notes[i].end = round(main_notes[i].end,2)\n",
        "    for i in range(len(acc_notes)):\n",
        "        acc_notes[i].start = round(acc_notes[i].start,2)\n",
        "        acc_notes[i].end = round(acc_notes[i].end,2)\n",
        "    main_notes.sort(key = lambda x:x.start)\n",
        "    acc_notes.sort(key = lambda x:x.start)\n",
        "    mpr = TMIDI.Tegridy_RPR_MidiEventProcessor()\n",
        "    repr_seq = mpr.encode(acc_notes)\n",
        "    total += len(repr_seq)\n",
        "    print('Converted file:', path)\n",
        "    print('Total INTs count:', len(repr_seq))\n",
        "    print('=' * 70)\n",
        "    return repr_seq\n",
        "\n",
        "def process_all_midis(midi_root, save_dir):\n",
        "    save_py = []\n",
        "    midi_paths = [d for d in os.listdir(midi_root)]\n",
        "    i = 0\n",
        "    out_fmt = '{}-{}.data'\n",
        "    for path in midi_paths:\n",
        "        pprint(path)\n",
        "        filename = midi_root + path\n",
        "        try:\n",
        "            data = process_midi(filename)\n",
        "        except KeyboardInterrupt:\n",
        "            print(' Abort')\n",
        "            return\n",
        "        except EOFError:\n",
        "            print('EOF Error')\n",
        "            return\n",
        "        save_py.append(data)\n",
        "    # pprint(save_py, compact=True)    \n",
        "    save_py = np.array(save_py, dtype='object')\n",
        "    print('=' * 70)\n",
        "    print('Total number of converted MIDIs:', save_py.size)\n",
        "    print('Total INTs count:', total)\n",
        "    np.save(save_dir + 'notes_representations.npy', save_py)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ5c6d5lXemo",
        "cellView": "form"
      },
      "source": [
        "#@title Process your custom MIDI DataSet :)\n",
        "%cd /content/\n",
        "import TMIDI\n",
        "\n",
        "%cd /content/MusicTransformer-Pytorch\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "%cd '/content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
        "\n",
        "custom_MIDI_DataSet_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
        "\n",
        "train_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/train' # split_type = 0\n",
        "test_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/test' # split_type = 1  \n",
        "val_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/val' # split_type = 2\n",
        "\n",
        "total_count = 0\n",
        "train_count = 0\n",
        "val_count   = 0\n",
        "test_count  = 0\n",
        "\n",
        "f_ext = '.pickle'\n",
        "fileList = os.listdir(custom_MIDI_DataSet_dir)\n",
        "for file in fileList:\n",
        "     # we gonna split by a random selection for now\n",
        "    \n",
        "    split = random.randint(1, 2)\n",
        "    if (split == 0):\n",
        "         o_file = os.path.join(train_dir, file+f_ext)\n",
        "         train_count += 1\n",
        "\n",
        "    elif (split == 2):\n",
        "         o_file0 = os.path.join(train_dir, file+f_ext)\n",
        "         train_count += 1\n",
        "         o_file = os.path.join(val_dir, file+f_ext)\n",
        "         val_count += 1\n",
        "\n",
        "    elif (split == 1):\n",
        "         o_file0 = os.path.join(train_dir, file+f_ext)\n",
        "         train_count += 1\n",
        "         o_file = os.path.join(test_dir, file+f_ext)\n",
        "         test_count += 1\n",
        "    \n",
        "    try:\n",
        "      \n",
        "      prepped = process_midi(file)\n",
        "\n",
        "      o_stream = open(o_file0, \"wb\")\n",
        "      pickle.dump(prepped, o_stream)\n",
        "      o_stream.close()\n",
        "\n",
        "      o_stream = open(o_file, \"wb\")\n",
        "      pickle.dump(prepped, o_stream)\n",
        "      o_stream.close()\n",
        "\n",
        "      print(file)\n",
        "      print(o_file)\n",
        "      print('Coverted!')\n",
        "\n",
        "    except KeyboardInterrupt: \n",
        "      raise\n",
        "\n",
        "    except:\n",
        "      print('Bad file. Skipping...', file)\n",
        "    #continue\n",
        "\n",
        "print('Done')\n",
        "print(\"Num Train:\", train_count)\n",
        "print(\"Num Val:\", val_count)\n",
        "print(\"Num Test:\", test_count)\n",
        "print(\"Total Count:\", train_count)\n",
        "\n",
        "%cd /content/MusicTransformer-Pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwCQIziNwHxe"
      },
      "source": [
        "#Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwisXl2Iy_Xf",
        "cellView": "form"
      },
      "source": [
        "#@title Activate Tensorboard Graphs/Stats to monitor/evaluate model perfomance during and after training runs\n",
        "# Load the TensorBoard notebook extension\n",
        "%reload_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "%tensorboard --logdir /content/MusicTransformer-Pytorch/rpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbv_sJyLq5om",
        "cellView": "form"
      },
      "source": [
        "#@title Start to Train the Model\n",
        "batch_size = 4 #@param {type:\"slider\", min:2, max:64, step:2}\n",
        "number_of_training_epochs = 600 #@param {type:\"slider\", min:10, max:600, step:10}\n",
        "maximum_output_MIDI_sequence = 2048 #@param {type:\"slider\", min:0, max:8192, step:128}\n",
        "!python3 train.py -output_dir rpr --rpr -batch_size=$batch_size -epochs=$number_of_training_epochs -max_sequence=$maximum_output_MIDI_sequence #-n_layers -num_heads -d_model -dim_feedforward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VLdhokSGUAu",
        "cellView": "form"
      },
      "source": [
        "#@title Re-Start Training from a certain checkpoint and epoch\n",
        "batch_size = 4 #@param {type:\"slider\", min:2, max:16, step:2}\n",
        "number_of_training_epochs = 400 #@param {type:\"slider\", min:10, max:600, step:10}\n",
        "maximum_output_MIDI_sequence = 2048 #@param {type:\"slider\", min:128, max:8192, step:128}\n",
        "saved_checkpoint_full_path = \"/content/MusicTransformer-Pytorch/rpr/weights/epoch_0145.pickle\" #@param {type:\"string\"}\n",
        "continue_epoch_number = 145 #@param {type:\"integer\"}\n",
        "\n",
        "!python3 train.py -output_dir rpr --rpr -batch_size=$batch_size -epochs=$number_of_training_epochs -max_sequence=$maximum_output_MIDI_sequence -continue_weights $saved_checkpoint_full_path -continue_epoch $continue_epoch_number #-n_layers -num_heads -d_model -dim_feedforward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1D-o-E-TnI8"
      },
      "source": [
        "###Evaluate the resulted models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MusrrrOxt1uy",
        "cellView": "form"
      },
      "source": [
        "#@title Graph the results\n",
        "import argparse\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RESULTS_FILE = \"results.csv\"\n",
        "EPOCH_IDX = 0\n",
        "LR_IDX = 1\n",
        "EVAL_LOSS_IDX = 4\n",
        "EVAL_ACC_IDX = 5\n",
        "\n",
        "SPLITTER = '?'\n",
        "\n",
        "\n",
        "def graph_results(input_dirs=\"/content/MusicTransformer-Pytorch/rpr/results\", output_dir=None, model_names=None, epoch_start=0, epoch_end=None):\n",
        "    \"\"\"\n",
        "    ----------\n",
        "    Author: Damon Gwinn\n",
        "    ----------\n",
        "    Graphs model training and evaluation data\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    input_dirs = input_dirs.split(SPLITTER)\n",
        "\n",
        "    if(model_names is not None):\n",
        "        model_names = model_names.split(SPLITTER)\n",
        "        if(len(model_names) != len(input_dirs)):\n",
        "            print(\"Error: len(model_names) != len(input_dirs)\")\n",
        "            return\n",
        "\n",
        "    #Initialize Loss and Accuracy arrays\n",
        "    loss_arrs = []\n",
        "    accuracy_arrs = []\n",
        "    epoch_counts = []\n",
        "    lrs = []\n",
        "\n",
        "    for input_dir in input_dirs:\n",
        "        loss_arr = []\n",
        "        accuracy_arr = []\n",
        "        epoch_count = []\n",
        "        lr_arr = []\n",
        "\n",
        "        f = os.path.join(input_dir, RESULTS_FILE)\n",
        "        with open(f, \"r\") as i_stream:\n",
        "            reader = csv.reader(i_stream)\n",
        "            next(reader)\n",
        "\n",
        "            lines = [line for line in reader]\n",
        "\n",
        "        if(epoch_end is None):\n",
        "            epoch_end = math.inf\n",
        "\n",
        "        epoch_start = max(epoch_start, 0)\n",
        "        epoch_start = min(epoch_start, epoch_end)\n",
        "\n",
        "        for line in lines:\n",
        "            epoch = line[EPOCH_IDX]\n",
        "            lr = line[LR_IDX]\n",
        "            accuracy = line[EVAL_ACC_IDX]\n",
        "            loss = line[EVAL_LOSS_IDX]\n",
        "\n",
        "            if(int(epoch) >= epoch_start and int(epoch) < epoch_end):\n",
        "                accuracy_arr.append(float(accuracy))\n",
        "                loss_arr.append(float(loss))\n",
        "                epoch_count.append(int(epoch))\n",
        "                lr_arr.append(float(lr))\n",
        "\n",
        "        loss_arrs.append(loss_arr)\n",
        "        accuracy_arrs.append(accuracy_arr)\n",
        "        epoch_counts.append(epoch_count)\n",
        "        lrs.append(lr_arr)\n",
        "\n",
        "    if(output_dir is not None):\n",
        "        try:\n",
        "            os.mkdir(output_dir)\n",
        "        except OSError:\n",
        "            print (\"Creation of the directory %s failed\" % output_dir)\n",
        "        else:\n",
        "            print (\"Successfully created the directory %s\" % output_dir)\n",
        "\n",
        "    ##### LOSS #####\n",
        "    for i in range(len(loss_arrs)):\n",
        "        if(model_names is None):\n",
        "            name = None\n",
        "        else:\n",
        "            name = model_names[i]\n",
        "\n",
        "        #Create and save plots to output folder\n",
        "        plt.plot(epoch_counts[i], loss_arrs[i], label=name)\n",
        "        plt.title(\"Loss Results\")\n",
        "        plt.ylabel('Loss (Cross Entropy)')\n",
        "        plt.xlabel('Epochs')\n",
        "        fig1 = plt.gcf()\n",
        "\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    if(output_dir is not None):\n",
        "        fig1.savefig(os.path.join(output_dir, 'loss_graph.png'))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    ##### ACCURACY #####\n",
        "    for i in range(len(accuracy_arrs)):\n",
        "        if(model_names is None):\n",
        "            name = None\n",
        "        else:\n",
        "            name = model_names[i]\n",
        "\n",
        "        #Create and save plots to output folder\n",
        "        plt.plot(epoch_counts[i], accuracy_arrs[i], label=name)\n",
        "        plt.title(\"Accuracy Results\")\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        fig2 = plt.gcf()\n",
        "\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    if(output_dir is not None):\n",
        "        fig2.savefig(os.path.join(output_dir, 'accuracy_graph.png'))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    ##### LR #####\n",
        "    for i in range(len(lrs)):\n",
        "        if(model_names is None):\n",
        "            name = None\n",
        "        else:\n",
        "            name = model_names[i]\n",
        "\n",
        "        #Create and save plots to output folder\n",
        "        plt.plot(epoch_counts[i], lrs[i], label=name)\n",
        "        plt.title(\"Learn Rate Results\")\n",
        "        plt.ylabel('Learn Rate')\n",
        "        plt.xlabel('Epochs')\n",
        "        fig2 = plt.gcf()\n",
        "\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    if(output_dir is not None):\n",
        "        fig2.savefig(os.path.join(output_dir, 'lr_graph.png'))\n",
        "\n",
        "    plt.show()\n",
        "graph_results('/content/MusicTransformer-Pytorch/rpr/results', model_names='rpr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxHrTsFUdn-r"
      },
      "source": [
        "To have the model continue your custom MIDI enter the following into the custom_MIDI field below:\n",
        "\n",
        "-primer_file '/content/some_dir/some_seed_midi.mid'\n",
        "\n",
        "For example: -primer_file '/content/MusicTransformer-Pytorch/seed.mid'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJXWoBMWL3ph"
      },
      "source": [
        "# Generate and Explore the output :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czNulONr4tB6",
        "cellView": "form"
      },
      "source": [
        "#@title Generate, Plot, Graph, Save, Download, and Render the resulting output\n",
        "number_of_tokens_to_generate = 2000 #@param {type:\"slider\", min:1, max:4096, step:1}\n",
        "priming_sequence_length = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "maximum_possible_output_sequence = 2048 #@param {type:\"slider\", min:0, max:4096, step:8}\n",
        "select_model = \"/content/MusicTransformer-Pytorch/rpr/results/best_loss_weights.pickle\" #@param [\"/content/MusicTransformer-Pytorch/rpr/results/best_acc_weights.pickle\", \"/content/MusicTransformer-Pytorch/rpr/results/best_loss_weights.pickle\"]\n",
        "custom_MIDI = \"\" #@param {type:\"string\"}\n",
        "%cd /content/MusicTransformer-Pytorch\n",
        "#import processor\n",
        "#from processor import encode_midi, decode_midi\n",
        "\n",
        "!python generate_TMIDI.py --rpr -output_dir output -model_weights=$select_model -target_seq_length=$number_of_tokens_to_generate -num_prime=$priming_sequence_length -max_sequence=$maximum_possible_output_sequence $custom_MIDI\n",
        "\n",
        "print('Successfully exported the output to output folder. To primer.mid and rand.mid')\n",
        "\n",
        "# set the src and play\n",
        "#FluidSynth(\"/content/font.sf2\").midi_to_audio('/content/MusicTransformer-Pytorch/output/rand.mid', '/content/MusicTransformer-Pytorch/output/output.wav')\n",
        "\n",
        "#from google.colab import files\n",
        "#files.download('/content/MusicTransformer-Pytorch/output/rand.mid')\n",
        "#files.download('/content/MusicTransformer-Pytorch/output/primer.mid')\n",
        "\n",
        "#Audio('/content/MusicTransformer-Pytorch/output/output.wav')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG48uyKGzcTI",
        "cellView": "form"
      },
      "source": [
        "#@title Plot and Graph the Output :)\n",
        "graphs_length_inches = 14 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "notes_graph_height = 10 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "highest_displayed_pitch = 92 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "lowest_displayed_pitch = 24 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "piano_roll_color_map = \"Blues\"\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import pypianoroll\n",
        "from pypianoroll import Multitrack, Track\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "#matplotlib.use('SVG')\n",
        "# For plotting\n",
        "import mir_eval.display\n",
        "import librosa.display\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "midi_data = pretty_midi.PrettyMIDI('/content/MusicTransformer-Pytorch/output/rand.mid')\n",
        "\n",
        "def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
        "    # Use librosa's specshow function for displaying the piano roll\n",
        "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
        "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
        "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
        "\n",
        "\n",
        "\n",
        "# Plot the output\n",
        "fname = '/content/MusicTransformer-Pytorch/output/rand'\n",
        "plt.figure(figsize=(graphs_length_inches, notes_graph_height))\n",
        "ax2 = plot_piano_roll(midi_data, int(lowest_displayed_pitch), int(highest_displayed_pitch))\n",
        "\n",
        "plt.show(block=False)\n",
        "FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
        "Audio(str(fname + '.wav'), rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}